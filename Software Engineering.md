# Software Engineering
Software Engineering is part of computer engineering that treats process of software development and maintenance.

### Evolutionary Improvement of Programs

Topic : Search Based Software Engineering

<https://dl.acm.org/doi/10.1109/TEVC.2010.2083669>

Genetic Programming is part of SBSE that evolve program with genetic algorithm to optimize some constraint.
This paper tries to optimize the code in terms of execution time, while preserving the semantic correctness.
The genetic algorithm initialized with given program, and genetic algorithm tries to evolve the program
with fitness function of elapsed time and test case correctness. 
To enhance the performance, the author co-evolved the test case population, and used multi objective optimization.
For the result, the program successfully optimized, that modern compilers fails to optimize.

### Evolving Human Competitive Spectra-Based Fault Localisation Techniques

Topic : Search Based Software Engineering

<https://link.springer.com/chapter/10.1007/978-3-642-33119-0_18>

Spectra-based Fault Localisation is part of software debugging techniques.
First, spectra is summary of test case execution, that stores number of correct execution, buggy execution,
correct non-execution, buggy non-execution, per each line of code.
Then SBFL tries to find some formula that orders each line of code by possibility of faultyness.
This paper tries to tackle this problem by genetically evolve function with fitness function as ranking of faulty line.
The result of evolution was human-competitive, in a sense that there were some formula that exceeds human designed formulas.
For the follow up study, some of formula was proven that it is optimal.

### “Sampling” as a Baseline Optimizer for Search-based Software Engineering

Topic : Search Based Software Engineering

<https://arxiv.org/pdf/1608.07617.pdf>

Search Based Software Enginnering views SE problems as mathematical optimization problems, and tries to find sub-optimal solution.
Usual approach is evolutionary algorithm or random search, where random search is well used as a baseline optimizer.
However, the problem of random search is that it is cost expensive, since we need to evaluate all generated sample.
This paper proposes 'SWAY', as a baseline optimizer for general SBSE problems.
Roughly, SWAY first generates large sized sample, divide it and compare each representative, then remove not selected.
The algorithm slightly differ whether target space is discrete or continuous.
If space is continous, it first get two farthest points, A, B, and divide space into component parallel to line AB.
If space is discrete, first approach fails since most of space is empty. Instead, first divide samples by number of 1 (selected number).
Now fix random point, and view distance between fixed point and each point as angular component, then divide sample after normalizing it.
If the space is highly constraint, sample is generated by SAT solver.
In result, SWAY was able to generate good enough solutions while having short runtime, with enough diversity.

### TypeWriter: Neural Type Prediction with Search-based Validation

Topic : Neural Type Inference

<https://arxiv.org/abs/1912.03768>

Dynamically typed languages like Python and javascript are well used, while type annotations are supported.
However, it is usual to omit type annotation, and deterministic type inference usually fail due to undecidability.
This paper proposes TypeWriter, consisting neural network model and typewriter, which finds correct type annotation among probabilistically given type annotations.
In specific, TypeWriter gives type inference for function argument and return type. 
The neural network uses four informations, name of variable, usage of variable, function comment, set of available types.
Each of informations are encoded by RNN, then concatenated, which is transformed to probabilisty for each types.
Now the typewriter part tries to find correct tuple of argument types and return types, by combinatorial search on top ranking elements.
The search is done with two fitness function, number of missing types and number of type errors. 
First, initialize the population with ranking-1 types. If it fails, we extend population with single element change.
For each generation, randomly select individual, and we extend population with single element change on this element.
If greedy option is turned on, we substitute population with that element. 
If any of element has fitness function 0, we return that element. Otherwise, repeat the loop.
In the result, the neural network itself has shown superior on other baselines, NL2Type, DeepTyper, sampler that samples one of 10-mostly-used types.
And the search problem, greedy algorithm gave 60~70% of success rate, and 50% of ground truth math.

### DeepXplore: Automated Whitebox Testing of Deep Learning Systems

Topic : Neural Network Testing

<http://www.cs.columbia.edu/~junfeng/papers/deepxplore-sosp17.pdf>

In the secure systems, neural network's correctness should be guaranteed, and was tested with random test and adversarial attack.
This paper asserts that these two tests are not enough to reveal fault in neural network, suggesting neuron coverage.
The neuron coverage is analogy of program coverage in classical software testing, where the neuron is activated if its value is higher than threshold for every test case.
With back propagation on neural network, DeepXplore algorithm performs gradient ascent on its test data so that it can maximize differential behavior and neuron coverage.
To maximize differentiable behavior, we choose one of DNN in SUT then maximize (\sum F_k(x)\[c\] - F_j(x)\[c\]), where F_k(x)\[c\] is probability that k-th neural network assigns class c to x. 
To maximize neuron coverage, we randomly inactivated neuron, and use its value directly to optimization function.
Also DeepXplore can add domain-specific constraint, for example in image datasets, paper only allowed modifying single small part of image, brightness of image, multiple tiny parts of image.
DeepXplore is experimented on five datasets, MNIST, ImageNet, Driving, Contagio/VirusTotal, Drebin with three DNNs each, and found thousands of differential examples.

### Guiding Deep Learning System Testing using Surprise Adequacy

Topic : Neural Network Testing

<https://arxiv.org/abs/1808.08444>

Classical neural network testing techniques relies on coverage of neurons, where these coverage measure does not involve relation to training data set.
This paper suggests new coverage measure, called surprise adequacy which measures how surprise the new input is with respect to traning set.
First, the activation trace is defined as activation values of subset of neurons and surprise adequecy is measured with new input's AT and training set's AT.
The research gives two kinds of surprise adequacy, likelihood based surprise adequacy and distance based surprise adequacy.
Likelihood based surprise adequecy measures sum of gaussian matrix kernel function between new input and training set, and distance based surprise adequecy measures ratio of distance to reference point and reference point's distance to classification boundary.
The paper gives four research questions, 1. Does SA really capture the surprise, 2. Does the selection of layer matters, 3. Is SA related to existing coverages, 4. Can SA improve accuracy against adversarial examples, and all four questions are answered positively.
